{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face x LangChain : A new partner package in LangChain\n",
    "langchain_huggingface, a partner package in LangChain jointly maintained by Hugging Face and LangChain. This new Python package is designed to bring the power of the latest development of Hugging Face into LangChain and keep it up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain_huggingface in c:\\users\\ishaan bhaskar\\appdata\\roaming\\python\\python311\\site-packages (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain_huggingface) (0.24.6)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\ishaan bhaskar\\appdata\\roaming\\python\\python311\\site-packages (from langchain_huggingface) (0.3.21)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\users\\ishaan bhaskar\\appdata\\roaming\\python\\python311\\site-packages (from langchain_huggingface) (3.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain_huggingface) (0.20.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain_huggingface) (4.45.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ishaan bhaskar\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\ishaan bhaskar\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.1.147)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\ishaan bhaskar\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.9.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\ishaan bhaskar\\appdata\\roaming\\python\\python311\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (8.5.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\ishaan bhaskar\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ishaan bhaskar\\appdata\\roaming\\python\\python311\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ishaan bhaskar\\appdata\\roaming\\python\\python311\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ishaan bhaskar\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\ishaan bhaskar\\appdata\\roaming\\python\\python311\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.8.30)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ishaan bhaskar\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.23.0->langchain_huggingface) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (4.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ishaan bhaskar\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.0.5)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ishaan bhaskar\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_huggingface) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\ProgramData\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface_hub in c:\\programdata\\anaconda3\\lib\\site-packages (0.24.6)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ishaan bhaskar\\appdata\\roaming\\python\\python311\\site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\ProgramData\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFaceEndpoint\n",
    "### How to Access HuggingFace Models with API\n",
    "There are also two ways to use this class. You can specify the model with the repo_id parameter. Those endpoints use the serverless API, which is particularly beneficial to people using pro accounts or enterprise hub. Still, regular users can already have access to a fair amount of request by connecting with their HF token in the environment where they are executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"?\\n\\nMachine learning is a method of teaching computers to learn from data, without being explicitly programmed. It involves creating algorithms that can automatically improve their performance on a specific task by learning from examples.\\n\\nThere are several types of machine learning, including supervised learning, unsupervised learning, and reinforcement learning. In supervised learning, the algorithm is trained on labeled data, which means the data is already categorized or labeled with the correct answer. In unsupervised learning, the algorithm is given unlabeled data and must find patterns and relationships on its own. Reinforcement learning involves an agent interacting with an environment and learning through trial and error.\\n\\nMachine learning has a wide range of applications, from image and speech recognition to natural language processing and predictive analytics. It is used in many industries, including healthcare, finance, and technology, to automate tasks, improve decision-making, and gain insights from large amounts of data.\\n\\nWhy is machine learning important?\\n\\nMachine learning is important because it allows computers to learn and improve on their own, without being explicitly programmed. This means that machine learning can be used to automate tasks that are too complex or time-consuming for humans to do manually.\\n\\nIn addition, machine learning can help businesses gain insights from large amounts of data, which can be used to make more informed decisions. For example, a retailer could use machine learning to analyze customer purchase data and identify patterns and trends, which could help them optimize their inventory and pricing strategies.\\n\\nMachine learning is also important because it has the potential to revolutionize many industries. For example, self-driving cars use machine learning to navigate roads and avoid obstacles, while medical imaging software uses machine learning to diagnose diseases more accurately.\\n\\nHow does machine learning work?\\n\\nMachine learning works by creating algorithms that can learn from data. These algorithms are designed to find patterns and relationships in the data, and to use that information to make predictions or decisions.\\n\\nThe first step in the machine learning process is to collect and preprocess the data. This may involve cleaning the data, normalizing it, and splitting it into training and testing sets.\\n\\nThe next step is to choose a machine learning model and to train it on the data. This involves feeding the data into the model and adjusting the model's parameters to minimize the error between the model's predictions and the actual outcomes.\\n\\nOnce the model is trained,\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and how can it be used to create content? Generative AI is a type of artificial intelligence that can generate new content, such as text, images, and music, by learning patterns from existing data. It works by using algorithms to analyze and generate content that is similar to the data it was trained on.\\n\\nOne common use of generative AI in content creation is to generate text. For example, a generative AI model might be trained on a large corpus of news articles, and then used to write new articles based on a given prompt. The model generates the text by predicting the next word based on the words that came before it, and continues to generate text until it reaches a predefined length or stops at a certain point.\\n\\nGenerative AI can also be used to create images. For example, a generative AI model might be trained on a large dataset of images of cats, and then used to generate new images of cats that are similar to the ones it was trained on. The model generates the images by predicting the next pixel based on the pixels that came before it, and continues to generate pixels until it has created a complete image.\\n\\nGenerative AI can also be used to create music. For example, a generative AI model might be trained on a large dataset of music, and then used to generate new music that is similar to the music it was trained on. The model generates the music by predicting the next note based on the notes that came before it, and continues to generate notes until it has created a complete piece of music.\\n\\nGenerative AI has many potential applications in content creation, and it is an active area of research. Some potential applications include creating personalized content for individuals, generating new ideas for creative professionals, and automating the creation of content for businesses and organizations. However, it is important to note that generative AI is not yet perfect, and it can sometimes generate content that is inappropriate or of low quality. It is also important to consider the ethical implications of using generative AI to create content, such as issues related to authorship, ownership, and the potential for misuse.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is generative AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='google/gemma-2-9b', temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 150, 'token': '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'}, model='google/gemma-2-9b', client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>, async_client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"google/gemma-2-9b\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "(Request ID: 92TpjCCNDpFPXv3fqmGhh)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nMake sure your token has the correct permissions.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ISHAAN BHASKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ISHAAN BHASKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/google/gemma-2-9b",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# we cant use some models like this\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is machine learning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ISHAAN BHASKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:390\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    387\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    388\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    402\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ISHAAN BHASKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:755\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    749\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    753\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    754\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ISHAAN BHASKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:950\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    936\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    937\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    938\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    948\u001b[0m         )\n\u001b[0;32m    949\u001b[0m     ]\n\u001b[1;32m--> 950\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ISHAAN BHASKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    791\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 792\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    793\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\Users\\ISHAAN BHASKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:779\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    771\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    776\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    778\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 779\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    783\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    786\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    787\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    788\u001b[0m         )\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\ISHAAN BHASKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1502\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1500\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1501\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1502\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1503\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1504\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1505\u001b[0m     )\n\u001b[0;32m   1506\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\ISHAAN BHASKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:312\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    311\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[1;32m--> 312\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ISHAAN BHASKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\inference\\_client.py:296\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 296\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\ISHAAN BHASKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:468\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m    463\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    466\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure your token has the correct permissions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    467\u001b[0m     )\n\u001b[1;32m--> 468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m416\u001b[39m:\n\u001b[0;32m    471\u001b[0m     range_header \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRange\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: (Request ID: 92TpjCCNDpFPXv3fqmGhh)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nMake sure your token has the correct permissions.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints)."
     ]
    }
   ],
   "source": [
    "# we cant use some models like this\n",
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "WARNING! token is not default parameter.\n",
      "                    token was transferred to model_kwargs.\n",
      "                    Please make sure that token is what you intended.\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, stop_sequences=[], server_kwargs={}, model_kwargs={'max_length': 150, 'token': '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'}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7,token=os.getenv(\"HF_TOKEN\"))\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] input_types={} partial_variables={} template=' \\nQuestion:{question}\\nAnswer:Lets think step by step.\\n'\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate,LLMChain\n",
    "template=\"\"\" \n",
    "Question:{question}\n",
    "Answer:Lets think step by step.\n",
    "\"\"\"\n",
    "\n",
    "prompt=PromptTemplate(template=template,input_variables=['question'])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ISHAAN BHASKAR\\AppData\\Local\\Temp\\ipykernel_11960\\677779490.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain=LLMChain(llm=llm,prompt=prompt)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'?\\n\\nIndia won the 2011 Cricket World Cup by defeating Sri Lanka by six wickets in the final. The match was played at the Wankhede Stadium in Mumbai, India on 2 April 2011.\\n\\nWhat was the final score of the 2011 Cricket World Cup?\\n\\nThe final score of the 2011 Cricket World Cup was India 277/4 (49.5 overs) vs. Sri Lanka 274/6 (50 overs). India won the match by six wickets.\\n\\nWhen was the 2011 Cricket World Cup held?\\n\\nThe 2011 Cricket World Cup was held from 19 February to 2 April 2011. The tournament was co-hosted by India, Bangladesh, and Sri Lanka.\\n\\nHow many teams participated in the 2011 Cricket World Cup?\\n\\nTen teams participated in the 2011 Cricket World Cup. These were the top eight teams from the ICC ODI rankings as of 31 August 2009, along with the three co-hosts.\\n\\nWhich players were named in the 2011 Cricket World Cup team of the tournament?\\n\\nThe team of the tournament for the 2011 Cricket World Cup was:\\n\\n* Kumar Sangakkara (Sri Lanka, wicket-keeper)\\n* Mahela Jayawardene (Sri Lanka)\\n* Sachin Tendulkar (India)\\n* Jacques Kallis (South Africa)\\n* Yuvraj Singh (India)\\n* Andrew Strauss (England, captain)\\n* Shane Watson (Australia)\\n* Lasith Malinga (Sri Lanka)\\n* Tillakaratne Dilshan (Sri Lanka)\\n* Saeed Ajmal (Pakistan)\\n* Zaheer Khan (India)\\n\\nKumar Sangakkara was named as the player of the tournament.\\n\\nWhere was the final match of the 2011 Cricket World Cup played?\\n\\nThe final match of the 2011 Cricket World Cup was played at the Wankhede'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
    "llm.invoke(\"Who won the cricket World cup 2011\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ISHAAN BHASKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9574fe06bbd402a8f8bf972b6a48c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ISHAAN BHASKAR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ISHAAN BHASKAR\\.cache\\huggingface\\hub\\models--BAAI--bge-small-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a1540b0c00491e9631d169c8d8561e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410b7d1dc88d4044936528d5e43dd199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/90.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d83c82b106648b3951c46819006f2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14b42705ebb4db1a2dd824db6f7a0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09eeb199ad1d44b98423a4b349f6d4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd77f773eabd47a9980fa9ab20fad80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bef558bcab43ffa4175ce9a2fb5aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ecef194aa2e4ad79ba2fc46a3165b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66a4e001db542359a205bc537c0e31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aeaee489a284d72b1836dcfee6f6d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = hf.embed_query(\"hi this is harrison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.028416553512215614,\n",
       " 0.012183267623186111,\n",
       " 0.027443964034318924,\n",
       " -0.0548286996781826,\n",
       " 0.024238882586359978,\n",
       " 0.0007663082797080278,\n",
       " 0.06783365458250046,\n",
       " 0.016348334029316902,\n",
       " -0.018950747326016426,\n",
       " 0.012542929500341415,\n",
       " 0.021565008908510208,\n",
       " -0.08793038129806519,\n",
       " 0.0006460736622102559,\n",
       " 0.03327082097530365,\n",
       " 0.005463734734803438,\n",
       " -0.06037648394703865,\n",
       " 0.05042264237999916,\n",
       " 0.00443480396643281,\n",
       " 0.0009599017212167382,\n",
       " 0.0017405530670657754,\n",
       " 0.003298826515674591,\n",
       " 0.03167249634861946,\n",
       " -0.048807498067617416,\n",
       " -0.044819146394729614,\n",
       " 0.07132109999656677,\n",
       " -0.0075108762830495834,\n",
       " -0.001125949784182012,\n",
       " -0.015801170840859413,\n",
       " -0.029402390122413635,\n",
       " -0.17224569618701935,\n",
       " -0.03189518302679062,\n",
       " -0.0016291674692183733,\n",
       " 0.018104977905750275,\n",
       " 0.015315400436520576,\n",
       " -0.020729567855596542,\n",
       " -0.00887299794703722,\n",
       " -0.0012822393327951431,\n",
       " 0.027276910841464996,\n",
       " -0.010114247910678387,\n",
       " 0.012621653266251087,\n",
       " -0.007077870890498161,\n",
       " -0.01669315993785858,\n",
       " 0.04085583984851837,\n",
       " 0.023938341066241264,\n",
       " -0.020081525668501854,\n",
       " 0.028681153431534767,\n",
       " -0.019400758668780327,\n",
       " -0.014618216082453728,\n",
       " 0.017379634082317352,\n",
       " 0.004164092242717743,\n",
       " 0.06415648013353348,\n",
       " 0.04768308624625206,\n",
       " 0.0018365149153396487,\n",
       " -8.069891191553324e-05,\n",
       " 0.016596801578998566,\n",
       " 0.011124172247946262,\n",
       " 0.06969437748193741,\n",
       " 0.05182051286101341,\n",
       " 0.055685292929410934,\n",
       " 0.055515438318252563,\n",
       " 0.000503968563862145,\n",
       " 0.04187062382698059,\n",
       " -0.15344089269638062,\n",
       " 0.05180783197283745,\n",
       " 0.006689795292913914,\n",
       " -0.03167073428630829,\n",
       " -0.009105022996664047,\n",
       " -0.05160471796989441,\n",
       " 0.04250858351588249,\n",
       " 0.028200050815939903,\n",
       " -0.010748136788606644,\n",
       " 0.02240579016506672,\n",
       " 0.044395506381988525,\n",
       " 0.004115525167435408,\n",
       " 0.01899850368499756,\n",
       " -0.004357183817774057,\n",
       " 0.047627631574869156,\n",
       " 0.011824628338217735,\n",
       " 0.00816461630165577,\n",
       " 0.008177281357347965,\n",
       " -0.009698772802948952,\n",
       " -0.014260279014706612,\n",
       " 0.0114096999168396,\n",
       " -0.07362118363380432,\n",
       " -0.05439520627260208,\n",
       " -0.05703961104154587,\n",
       " -0.003608557628467679,\n",
       " 0.0026660789735615253,\n",
       " 0.023782461881637573,\n",
       " 0.015376233495771885,\n",
       " -0.07020372897386551,\n",
       " -0.031300339847803116,\n",
       " -0.0031142919324338436,\n",
       " -0.015812164172530174,\n",
       " -0.03791401907801628,\n",
       " -0.025921903550624847,\n",
       " 0.018168414011597633,\n",
       " -0.03882461041212082,\n",
       " -0.056745078414678574,\n",
       " 0.5792059898376465,\n",
       " -0.05278833210468292,\n",
       " 0.020716389641165733,\n",
       " 0.06794387102127075,\n",
       " -0.04541655257344246,\n",
       " 0.011642470955848694,\n",
       " -0.02157178334891796,\n",
       " 0.02034173533320427,\n",
       " -0.027448926120996475,\n",
       " -0.045588959008455276,\n",
       " -0.029443582519888878,\n",
       " -0.02366250567138195,\n",
       " -0.034315258264541626,\n",
       " 0.001938889967277646,\n",
       " -0.07095137983560562,\n",
       " 0.034556321799755096,\n",
       " -0.030558936297893524,\n",
       " 0.03907858580350876,\n",
       " -0.02970731444656849,\n",
       " -0.0008283082279376686,\n",
       " -0.01215936616063118,\n",
       " -0.018272824585437775,\n",
       " 0.025486504659056664,\n",
       " -0.0044616470113396645,\n",
       " 0.01633535325527191,\n",
       " 0.019126499071717262,\n",
       " -0.05483205243945122,\n",
       " 0.027635985985398293,\n",
       " -0.004757674410939217,\n",
       " 0.059001725167036057,\n",
       " -0.0016944637754932046,\n",
       " 0.008014998398721218,\n",
       " -0.03772684186697006,\n",
       " -0.09893044084310532,\n",
       " -0.022574376314878464,\n",
       " -0.037604641169309616,\n",
       " -0.002169858431443572,\n",
       " 0.00324462354183197,\n",
       " -0.01920255273580551,\n",
       " -0.008631214499473572,\n",
       " -0.04802308604121208,\n",
       " 0.008696692064404488,\n",
       " -0.09516111016273499,\n",
       " -0.03496047854423523,\n",
       " -0.04360801354050636,\n",
       " -0.00034402511664666235,\n",
       " -0.010173643939197063,\n",
       " -0.030999548733234406,\n",
       " 0.02430969849228859,\n",
       " -0.020402032881975174,\n",
       " 0.031139390543103218,\n",
       " 0.0008811151492409408,\n",
       " 0.013916471041738987,\n",
       " -0.03119622729718685,\n",
       " -0.037154052406549454,\n",
       " 0.0040296344086527824,\n",
       " 0.014799810945987701,\n",
       " 0.04318896681070328,\n",
       " 0.03875482454895973,\n",
       " 0.013851989060640335,\n",
       " 0.01979786343872547,\n",
       " 0.010267077013850212,\n",
       " -0.005434108432382345,\n",
       " -0.014299245551228523,\n",
       " 0.02763785608112812,\n",
       " 0.009802667424082756,\n",
       " -0.1355028599500656,\n",
       " -0.017139770090579987,\n",
       " 0.017617112025618553,\n",
       " 0.023132260888814926,\n",
       " 0.001759014674462378,\n",
       " 0.03088941052556038,\n",
       " 0.03991871327161789,\n",
       " -0.013684175908565521,\n",
       " 0.024816518649458885,\n",
       " 0.05405018851161003,\n",
       " 0.017761148512363434,\n",
       " -0.01847507618367672,\n",
       " 0.025955377146601677,\n",
       " -0.0063775465823709965,\n",
       " -0.016587326303124428,\n",
       " 0.03784804418683052,\n",
       " -0.027290022000670433,\n",
       " -0.05284580588340759,\n",
       " -0.03803317993879318,\n",
       " 0.05191105604171753,\n",
       " -0.007557093631476164,\n",
       " -0.031805310398340225,\n",
       " 0.01328420639038086,\n",
       " -0.02772371843457222,\n",
       " 0.05630653724074364,\n",
       " 0.003041862277314067,\n",
       " 0.05332483723759651,\n",
       " -0.05791125446557999,\n",
       " -0.011325802654027939,\n",
       " -0.03117201291024685,\n",
       " 0.02560868300497532,\n",
       " 0.03389059752225876,\n",
       " -0.0010284296004101634,\n",
       " 0.015864890068769455,\n",
       " 0.010595209896564484,\n",
       " -0.02703780308365822,\n",
       " -0.0009308418957516551,\n",
       " -0.04815222695469856,\n",
       " 0.0281792264431715,\n",
       " 0.010320610366761684,\n",
       " 0.06662959605455399,\n",
       " -0.016558198258280754,\n",
       " -0.004431403707712889,\n",
       " 0.03823426365852356,\n",
       " -0.023408204317092896,\n",
       " -0.035581767559051514,\n",
       " -0.05829068273305893,\n",
       " -0.011181488633155823,\n",
       " -0.01768455095589161,\n",
       " -0.016141287982463837,\n",
       " -0.0342453271150589,\n",
       " -0.025139527395367622,\n",
       " 0.039396680891513824,\n",
       " -0.02365821786224842,\n",
       " -0.007725023664534092,\n",
       " -0.005098918452858925,\n",
       " -0.035234373062849045,\n",
       " -0.014076837338507175,\n",
       " -0.223260298371315,\n",
       " -0.03147139027714729,\n",
       " -0.0012906095944344997,\n",
       " -0.0017199853900820017,\n",
       " -0.007846038788557053,\n",
       " -0.05802326649427414,\n",
       " 0.046174563467502594,\n",
       " 0.024552641436457634,\n",
       " 0.07320838421583176,\n",
       " 0.017268331721425056,\n",
       " 0.047612059861421585,\n",
       " 0.013473288156092167,\n",
       " -0.005516040604561567,\n",
       " -0.014357835985720158,\n",
       " -0.009674296714365482,\n",
       " 0.04878249019384384,\n",
       " 0.030538106337189674,\n",
       " -0.024993961676955223,\n",
       " 0.021486228331923485,\n",
       " 0.01763981580734253,\n",
       " 0.05313895270228386,\n",
       " 0.013485005125403404,\n",
       " -0.023225966840982437,\n",
       " -0.021403975784778595,\n",
       " 0.026075344532728195,\n",
       " 0.0020292021799832582,\n",
       " 0.12753744423389435,\n",
       " 0.08316831290721893,\n",
       " 0.04408948868513107,\n",
       " -0.02670358307659626,\n",
       " 0.005522021092474461,\n",
       " -0.009294888935983181,\n",
       " 0.020074322819709778,\n",
       " -0.09684176743030548,\n",
       " -0.02470395527780056,\n",
       " 0.025086961686611176,\n",
       " 0.002088603563606739,\n",
       " -0.04489403963088989,\n",
       " -0.07861138135194778,\n",
       " -0.004376337863504887,\n",
       " -0.06590455025434494,\n",
       " 0.014689413830637932,\n",
       " -0.05764186009764671,\n",
       " -0.07152027636766434,\n",
       " -0.06232647970318794,\n",
       " 0.0034316712990403175,\n",
       " -0.046065520495176315,\n",
       " 0.04530090093612671,\n",
       " -0.026762256398797035,\n",
       " 0.03401093930006027,\n",
       " 0.045473817735910416,\n",
       " -0.02817925252020359,\n",
       " 0.005011804401874542,\n",
       " 0.009630844928324223,\n",
       " -0.030305562540888786,\n",
       " -0.03612479940056801,\n",
       " -0.013627000153064728,\n",
       " -0.032653726637363434,\n",
       " -0.04467754811048508,\n",
       " 0.010642170906066895,\n",
       " -0.027486354112625122,\n",
       " -0.02456510439515114,\n",
       " -0.02474776841700077,\n",
       " 0.05361954867839813,\n",
       " 0.020789964124560356,\n",
       " 0.01946849375963211,\n",
       " 0.05324122682213783,\n",
       " -0.014002456329762936,\n",
       " 0.021243249997496605,\n",
       " -0.04957324638962746,\n",
       " -0.008522586897015572,\n",
       " 0.007852885872125626,\n",
       " -0.05719393491744995,\n",
       " -0.027550628408789635,\n",
       " 0.005300895310938358,\n",
       " 0.04007291421294212,\n",
       " 0.019597919657826424,\n",
       " -0.04519734904170036,\n",
       " 0.03243580833077431,\n",
       " -0.012342418543994427,\n",
       " 0.03431442007422447,\n",
       " 0.021102122962474823,\n",
       " 0.03984649479389191,\n",
       " 0.03166380152106285,\n",
       " -0.03359023854136467,\n",
       " 0.03164784237742424,\n",
       " -0.0033045399468392134,\n",
       " 0.00464182673022151,\n",
       " 0.037589337676763535,\n",
       " -0.059244606643915176,\n",
       " 0.007028385065495968,\n",
       " 0.003808708628639579,\n",
       " -0.025788886472582817,\n",
       " -0.021203340962529182,\n",
       " 0.02269122377038002,\n",
       " -0.02177296206355095,\n",
       " -0.27963781356811523,\n",
       " 0.007267402485013008,\n",
       " 0.0210720244795084,\n",
       " 0.04519748315215111,\n",
       " -0.02053448185324669,\n",
       " 0.02431369759142399,\n",
       " -0.0006137037998996675,\n",
       " -0.01185703556984663,\n",
       " -0.03296775743365288,\n",
       " 0.03584311157464981,\n",
       " 0.03128174692392349,\n",
       " 0.06373952329158783,\n",
       " 0.04654783755540848,\n",
       " -0.014470522291958332,\n",
       " 0.015869710594415665,\n",
       " 0.03397125378251076,\n",
       " 0.01805957593023777,\n",
       " 0.0022987546399235725,\n",
       " 0.01654987223446369,\n",
       " -0.021714884787797928,\n",
       " -0.03485995903611183,\n",
       " -0.0008649451192468405,\n",
       " 0.15126043558120728,\n",
       " -0.024536754935979843,\n",
       " 0.030671218410134315,\n",
       " -0.00731823081150651,\n",
       " -0.0061354562640190125,\n",
       " 0.06415145099163055,\n",
       " 0.01602148450911045,\n",
       " -0.03636445105075836,\n",
       " 0.019898604601621628,\n",
       " -0.021172352135181427,\n",
       " 0.048294126987457275,\n",
       " -0.04478098452091217,\n",
       " 0.0476338230073452,\n",
       " 0.000774955959059298,\n",
       " -0.005927947349846363,\n",
       " 0.06154267489910126,\n",
       " 0.023968402296304703,\n",
       " 0.01330501213669777,\n",
       " 0.02268449403345585,\n",
       " 0.01453809067606926,\n",
       " -0.05215905234217644,\n",
       " -0.03274965658783913,\n",
       " 0.08583346009254456,\n",
       " -0.0037248358130455017,\n",
       " 0.0013494329759851098,\n",
       " 0.040919892489910126,\n",
       " 0.011659668758511543,\n",
       " 0.058436185121536255,\n",
       " -0.022286172956228256,\n",
       " -0.011520695872604847,\n",
       " 0.0047057149931788445,\n",
       " 0.04718261584639549,\n",
       " -0.0019179013324901462,\n",
       " 0.0330093614757061,\n",
       " -0.03505057469010353,\n",
       " -0.0207365732640028,\n",
       " -0.009222202003002167,\n",
       " 0.014618288725614548,\n",
       " 0.006456065457314253,\n",
       " 0.0010978657519444823,\n",
       " 0.010224000550806522,\n",
       " 0.08537217974662781,\n",
       " 0.03883953392505646]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
